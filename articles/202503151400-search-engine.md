
# Building a Search Engine from Scratch in Rust

In January of this year, I was hired by a company dedicated to keeping client data private. To achieve this, they encrypt as much data as possible, making efficient search functionality a significant challenge.

## The Challenge

The core issue was simple: **How do you allow users to search through their encrypted data while ensuring it remains private?**
Standard search solutions were not an option, as they typically require decrypting data on the server—something we couldn’t allow.

## Key Requirements

- **Privacy-first indexing**: Once data is indexed, it should remain encrypted on disk, preventing content extraction through index analysis.
- **On-device search**: Since decrypting data on the server was not viable and homomorphic search is not yet practical, all indexing and searching had to happen directly on the user's device.
- **Minimal resource footprint**: The search engine had to work efficiently across a wide range of devices, from low-end Android phones to high-end MacBooks, optimizing CPU, memory, and binary size.
- **Cross-platform compatibility**: It needed to run seamlessly in browsers, mobile apps, and desktop applications.
- **Network-efficient synchronization**: To avoid redundant indexing, devices had to share indexes over the network, even under poor connection conditions. This required implementing sharded indexes to sync only updated segments.

## Why Existing Solutions Didn't Work

1. **Meilisearch, Elasticsearch**: Store data in plaintext on the server, violating our privacy constraints.
2. **Tantivy**: Relies heavily on threads, making it incompatible with browsers. Removing threading would require a major rewrite, which wasn’t practical.
3. **SQLite + SQLCipher**: Lacked sharding support, resulted in a bloated binary, and posed storage challenges in a browser environment.

With no suitable off-the-shelf solution, I decided to build a custom search engine from scratch—within a tight 2.5-month deadline.

## Architecture Overview

### Storage Layer

Given the predefined requirements, the sharded indexes need to be stored on a filesystem. While this is straightforward for mobile and desktop applications, browsers introduce additional challenges.

Modern browsers provide an embedded [Filesystem API](https://developer.mozilla.org/en-US/docs/Web/API/File_System_API). However, since file access is asynchronous in browsers, all filesystem interactions—whether on desktop, mobile, or browser—must be handled asynchronously to maintain a unified codebase.

One key limitation of the Filesystem API is that, as of today, **the same file cannot be opened multiple times simultaneously**, even in read-only mode.

#### Async Filesystem Layer

- The [`async-fs`](https://crates.io/crates/async-fs) crate (by the `smol` team) provides an efficient solution.
- Initially, I considered the [`web-fs`](https://crates.io/crates/web-fs) crate for browser support, but it was designed to run in the **main thread**, whereas the Filesystem API is only accessible from a **worker thread**.

To improve performance, I developed [`browser-fs`](https://crates.io/crates/browser-fs), a lightweight alternative that removes this extra abstraction, leading to better efficiency.

---

### Encryption Layer

Since all stored data must be encrypted, an encryption layer sits on top of the storage layer. The encryption is handled using **AES-GCM**, which meets all necessary security requirements.

For browsers, leveraging the **built-in Web Crypto API** instead of a Rust-based implementation significantly reduces the binary size. Additionally, offloading encryption and decryption to the browser results in substantial performance gains.

To optimize this, I developed [`browser-crypto`](https://crates.io/crates/browser-crypt), which integrates seamlessly with the browser’s native encryption capabilities. Benchmarks showed that encrypting **1MB of binary data was 2–3× faster** when using the browser’s implementation rather than a pure Rust solution.

---

### Indexing and Collection Management

The search engine is designed for multiple products with varying data structures. Some products may need to index **booleans, numbers, text, or tags**, but data volume will differ across use cases. The indexing system is designed to:

1. **Keep index files small**.
2. **Minimize unnecessary updates** by only modifying affected files.
3. **Ensure efficient synchronization across devices**.

#### **File Structure**

Each dataset is structured as follows:

- **Manifest File**: Stores metadata about all shards, including the total number of documents across them.
- **For each shard**:
  - **Collection File**: Contains document identifiers and their attribute definitions.
  - **Index Files**: Separate index files for different attribute types (e.g., booleans, integers, text, coordinates).

This structure ensures:
- Only relevant files are created for each dataset.
- If a document’s **text attribute** is updated, only the corresponding **text index file** is modified—reducing synchronization overhead.

#### **Sharding Mechanism**

The **sharding strategy is defined by the user** when setting up the document schema. The user selects an integer-based attribute as the **shard key**, which determines how documents are split across shards.

When a shard reaches its size limit, it automatically **splits into two**, ensuring scalability without requiring a complete reindexing.

For example, let's assume we have a file size limit to 1MB and we have the following files

```
shard 0: (shard key: [0-100])
  - collection: 0.2M (filename: `aaaaaa`)
  - text-index: 0.9M (filename: `aaaaac`)
  - integer-index: 0.2M (filename: `aadaaa`)
```

If we add a document containing a lot of text, the shard will be split in to, distributing the collection content across both shards

```
shard 0: (shard key: [0-50])
  - collection: 0.1M (filename: `aaaaaa`)
  - text-index: 0.5M (filename: `aaaaac`)
  - integer-index: 0.1M (filename: `aadaaa`)
shard 1: (shard key: [50-100])
  - collection: 0.1M (filename: `aaaaba`)
  - text-index: 0.5M (filename: `baaaac`)
  - integer-index: 0.1M (filename: `acdaaa`)
```

For example, when indexing a **conversation system**, the shard key could be the **message creation date**. This ensures:
- Messages from a similar time range stay within the same shard.
- Searches for recent messages can be optimized by querying the latest shards first, improving performance.

---

### Collection and index architecture

#### Collection

As described earlier, the collection contains all the documents identifiers that have been indexes and the attributes used in by the indexed documents. These information are being stored on disk as follow. To keep things easy to read, I will use a json like representation.

```json
{
  "attributes": [
    { "index": 0, "name": "sender" },
    { "index": 1, "name": "content" },
    { "index": 2, "name": "recipient" },
  ],
  "entries": [
    { "index": 0, "name": "000000", "shard": 42 },
    { "index": 4, "name": "000000", "shard": 43 }
  ]
}
```

But once deserialized, they are stored in memory in the following structure

```rust
struct Collection {
    attributes_by_index: BTreeMap<AttributeIndex, Arc<str>>,
    attributes_by_index: HashMap<Arc<str>, AttributeIndex>,
    entries_by_index: BTreeMap<EntryIndex, Entry>,
    entries_by_name: HashMap<Arc<str>, EntryIndex>,
    entries_by_partition: BTreeMap<u64, BTreeSet<EntryIndex>>,
}
```

Where `AttributeIndex` and `EntryIndex` are the indexes, in the collection, of an attribute and an entry. In order to optimise the memory and storage usage, when inserting a document in the index, an index (`EntryIndex`) is affected to that entry, which is then used in the other indexes instead of using the name of the entry. A `String` has a size of 24 bytes plus the length of the content. An `Arc<str>` is 16 bytes plus the length of the content, but only once if cloned. By using an `EntryIndex`, which is a wrapper around `u16`, it only uses 2 bytes to refer to an entry. This assumes that a collection will have, at most, 65536 entries. The `AttributeIndex` is a wrapper around `u8`, limiting the number of attributes to 256, which is probably fair to assume.

Those entry and attribute indexes are then used by the indexes to point to the specific data.

#### Indexes

The index architecture is designed in a way that, in the less iteration possible, it's possible to find a result. Therefore, considering we usually look for a term, for a given attribute, or not, the content of the index is usually wrapped as follow

```
{
  "content": {
    <term_000>: {
      <AttributeIndex>: {
        <EntryIndex>: <position>
      }
    }
  }
}
```

That way, when looking for all the entries that contain the term `term_000` for the attribute `attribute_0`, the solution is only 2 accesses away. And when we search for any attribute, considering the limited number of attributes, accessing all the possible entries only requires to go through all the attributes.

The `position` refers to where the term can be found in the attribute of the given entry. Considering, and entry can have multiple values for the same attribute, it mostly consists of an index in that list of values (encoded as a `u8`, so limited to 256 values).

#### Text index

The **text index** is a bit particular: when a string is indexed, it's first preprocessed to tokenize and stem the tokens. That way, for the given `"Such a wonderful day!"`, one value will contain `["such", "wonderful", "day"]`. But to provide some useful insights in the search results, we need to keep the index of each token in the original string (`such: 0` , `wonderful: 1` and `day: 2`) but also the position in the original string (`such: 0`, `wonderful: 7` and `day: 17`) so that the user can highlight the reason of the result.

Which means, each token will be represented by `ValueIndex` (as a `u8`), `TokenIndex` (as a `u16`) and a `CharIndex` (as a `u32` considering the 4GB limitation of WASM).

---

### Searching through the Indexed Data

Searching simply involves iterating through each shard, loading the collection file and necessary indexes, computing a score for each document, and aggregating the results from each shard.

A search request can be defined and executed as follow

```rust
// Giving a structured query
let filter = Filter::and()
    .with(Condition::Any(TextFilter::Matches("hello")))
    .with(Filter::or()
        .with(Condition::Field("sender", TagFilter::Is("Alice")))
        .with(Condition::Field("sender", TagFilter::Is("Bob"))));
let query = engine.query().build(filter)?;
// Parsing a query
let query = engine.query().parse("\"hello\" and (sender:\"Alice\" OR sender:Bob)")?;
// And then executed
let result = query.execute(|event| {
    println!("found: {event:?}");
}).await?;
```


#### Scoring System

To ensure consistent scoring across all shards, I used the [**Okapi BM25**](https://en.wikipedia.org/wiki/Okapi_BM25) algorithm. However, due to the absence of shared word counts across the engine, the scoring assumes the words have the same frequency across all shards.

When aggregating scores on queries using `or` and `and`, I initially used respectively `max` and `min` but the accuracy of the results where not good enough. In the first case, it was too loose and in the second, too strict.

For more accurate scoring when combining results with `or` and `and` operations, I introduced configurable constants for each operation:

- `bm25_and(first, second) = alpha * min(first, second) + (1.0 - alpha) * max(first, second)`: This allows some flexibility in case one term is slightly weaker.
- `bm25_or(first, second) = beta * max(first, second) + (1.0 - beta) * avg(first, second)`: This ensures that a single high-BM25 term doesn’t completely dominate.

---

### Indexing Documents

When creating an engine, the user defines the structure for every document. Here's an example schema definition in Rust:

```rust
let schema = Schema::builder()
    .attribute("sender", Attribute::Tag)
    .attribute("recipient", Attribute::Tag)
    .attribute("content", Attribute::Text)
    .attribute("date", Attribute::Integer)
    .sharding_key("date");
```

Documents are validated against this schema when added to the index:

```rust
let document = Document::new("identifier")
    .with("sender", "Alice")
    .with("recipient", "Bob")
    .with("content", "Hello World!")
    .with("date", 123456789);
writer.insert(document).await?;
```

Each document attribute can contain multiple values, like multiple recipients in this case.

Representing the content of that document as a JSON would look like

```json
{
  "id": "identifier",
  "attributes": {
    "sender": ["Alice"],
    "recipient": ["Bob"],
    "content": ["Hello World!"],
    "date": [123456789]
  }
}
```

#### Transaction Management

To handle document insertion safely (especially in case of browser tab closure), I implemented a transaction system. When a user updates the engine, a transaction is created containing a copy of the **manifest file** and a list of the modified files. Upon committing, the manifest is updated, pointing to the newly created files, ensuring atomicity and recovery in case of failure.

During a commit we have the following state.
```
manifest:
  - shard 0:
    - collection: aaaaaa
    - text_index: aaaaab
    - integer_index: aaaaac
  - shard 1:
    - collection: aaaaad
    - text_index: aaaaae
    - integer_index: aaaaaf

transaction manifest:
  - shard 0:
    - collection:
        base: aaaaaa
        new: aaaaba # has been updated
    - text_index:
        base: aaaaab
        new: aaaabb # has been updated
    - integer_index:
        base: aaaaac
  - shard 1:
    - collection:
        base: aaaaad
    - text_index:
        base: aaaaae
    - integer_index:
        base: aaaaaf
  - shard 2: # new shard
    - collection:
        new: aaaaca
    - text_index:
        new: aaaacb
    - integer_index:
        new: aaaaacc
```

After committing, we get the following state

```
manifest:
  - shard 0:
    - collection: aaaaba
    - text_index: aaaabb
    - integer_index: aaaaac
  - shard 1:
    - collection: aaaaad
    - text_index: aaaaae
    - integer_index: aaaaaf
  - shard 2:
    - collection: aaaaca
    - text_index: aaaacb
    - integer_index: aaaaacc
```

This mechanism provides a way to the user to keep executing queries even during a transaction, and only blocks the search when the new manifest is being written.

---

## Overall usage

With all those building blocks in place, using the search engine is as simple as the following example

```rust
let schema = Schema::builder()
    .attribute("sender", Attribute::Tag)
    .attribute("recipient", Attribute::Tag)
    .attribute("content", Attribute::Text)
    .attribute("date", Attribute::Integer)
    .sharding_key("date")?;

let engine = Engine::builder()
    .schema(schema)
    .storage("/path/to/index")
    .cipher_key([42; 32])
    .build().await?;

let mut tx = engine.transaction().await?;
let document = Document::new("identifier")
    .with("sender", "Alice")
    .with("recipient", "Bob")
    .with("content", "Hello World!")
    .with("date", 123456789);
tx.insert(document).await?;
tx.commit("inserted my first document").await?;

let query = engine.query().parse("recipient:Bob")?;
let result = query.execute(|event| {
    println!("found: {event:?}");
}).await?;
println!("hit: {}", result.hit);
println!("duration: {}", result.duration);
```

---

## Performance Optimization

- **Sharded Files**: Loading encrypted sharded files can take time. To mitigate this, I implemented a cache layer, which keeps the files in memory until the cache size is exceeded.
- **Document Presence Check**: To efficiently check if a document is indexed, I implemented a [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter) for each shard, reducing the need for expensive disk I/O operations.

## Conclusion

After 2 months of development, the search engine was up and running, ready to be integrated into a browser and mobile application. However, the implementation of the synchronization layer was still in progress, which may be the topic of a future article.

Key performance metrics:
- **1GB of data indexed in 15 minutes**.
- **Search queries return results in under 1 second**.
